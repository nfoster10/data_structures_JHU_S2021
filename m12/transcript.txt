 >> Welcome to the second portion of the module on searching. So far, we've looked at sequential search, which is a naive strategy. Naive is a strategy that someone might reasonably think of even if they had no training in such strategies. Binary search is a strategy that we looked at when we were talking about recursion. This time we reviewed it from the point of view of a searching technique. It's suitable for small quantities of data that fit nicely into random access storage. We looked at Interpolation search, which is a more efficient strategy than its cousin binary search. The problem is, it requires an assumption of uniform distribution on the data, and many times we don't know the distribution and can't make that assumption, so this strategy doesn't get used as often as we might like to. Index sequential search introduces the concept of an index, which allows you to focus your search region. Finally, we come to search trees: the remaining technique that relies on sorted data. The key thing that you need to keep in mind when you're talking about strategies that rely on sorted data, is that there's a maintenance cost that comes with having the data in sorted order. Every time you do an Insert or Delete, you have to make sure that you maintain the file in sorted order. Otherwise, you lose your ability to leverage the order. That requires additional work. You have to think about the cost of the insertion and the cost of deletion when you're choosing a strategy that's based on sorted order. There might be an argument for saying, "Hey, I really need to do very few insertions and deletions, so It makes sense to maintain the file in sorted order." You'll see in practice, however, that we often maintain things in sorted order and just eat the cost of doing insertions or deletions. Onward with search trees. A search tree is a binary tree or a higher-order tree that has the search tree characteristic. We identified that back in the binary tree module. A search tree characteristic is a recursive characteristic in which everything in the left sub-tree of the root is smaller than the root, and everything in the right sub-tree is greater than or equal to the root. Now you first saw this with our strategy for building a general sorting tree. We built a tree that turned out to have a search tree characteristic, and when we did the inorder traversal, we got the sorted list. That will be one way that we can verify that a tree has search tree characteristic,is that the inorder traversal will give us the sorted list very closely tied together. If we have a tree that has this search tree characteristic, and keep in mind, this has to be recursively true throughout the entire tree, then that gives us a structure that represents a nice compromise, because we can get to an individual record very efficiently. Start over. Having a search tree can be a nice compromise when you need to utilize the file in different ways. Accessing an individual record is efficient because each individual record can be found on a path from the root to a leaf. The length of the longest search path in a tree is the height of the tree. If I want to access the entire file in some way, like I want to put out a directory of all the people at my database, I do my inorder traversal, I get the sorted list and I'm ready to go. I have fast efficiency for perhaps a very common operation of accessing individual records, but I don't give up the ability to do something with the entire file. In order for my individual accesses to be efficient, the tree needs to be as short and bushy as possible for the number of items. You can see we're throwing this term balanced around as well. We need to think about what we mean by the term balanced. Intuitively, it is going to mean that both sides of the tree are similar in size and shape. We also want the tree to be a short bushy as possible for the number of data items so that we have the shortest path to any particular data item. Log n is typically the height. Start it over. Stop a minute and think about what the cost of accessing an individual item is going to be in a tree where the cost is a function of the height of the tree. The length of the longest path from the root to a leaf. Ready? Okay, those of you that thought log n, you're right on target. Basically going to talk about three types of search trees. We'll talk about a very simple strategy that's easy to do, but has less than perfect performance, we'll briefly discuss an alternative method that has very good performance, then we'll go into some detail on a third method that works well with higher-order trees. Simple search trees: we start with a sorted file of data. Notice right off the bat, this is very different from the method that we used to talking about sort trees where we use your random file >> For the time being, we want our concept of balance to be simple and intuitive. We just want both sides of the tree, both subtrees to be reasonably similar to each other. Our maintenance approach is going to be somewhat relaxed. We're not going to worry too much about maintaining the shape of the tree or maintaining the balance. The side effect of that relaxed attitude will be that over time, as we do insertions and deletions to the tree. It will become less balance, less bushy, more generalized, more stringy and drawn out vertically. As a consequence of all of that, less efficient to search. From a theoretical standpoint, it turns out that unless it's really, really bad, it's still going to be log n but it will definitely add additional comparisons to find a particular item. Our strategy this time is to start with the resorted file and this process is recursive. We will use the middle item as the root of the tree. As you may recall before, we use the first item as the root of the tree. To the left half will become the left subtree and the right half will become the right subtree. Here's an example. We're starting with a sorted list as you can see. On the left, you can see the type of tree we would get if we use the procedure that we used from the material on sorting. Clearly that would be very undesirable tree. It's basically a linked list. On the right we see the tree that we do create using this method. Right after that, you see that given the number of data items in the file, it's a short and as bushy as it can possibly be. Every item is as close to the root as they could be. There are other trees that are equally as good but there's nothing that is better than this in terms of shortness and bushiness. Let's work through the example and see how the tree is constructed. 25 becomes the root and 12, 15, 16, and 22 becomes the left subtree. 42 up to 96 becomes the right subtree. This is a recursive process. We find the middle item of the right-hand half to continue the example, 70 becomes the root of the right-hand half, 42 and 43 are the left sub-tree, 80 and 96 are the right subtree. How do we turn 80 and 96 into the right subtree? We first start by finding [NOISE] the middle item. There are 11 items in this array. If I were to write the indices above them, just to simplify things a little bit. It's like 10 items, sorry about that. 9 plus 10 is 19 divided by 2, gives me 9.5 is the index of the midpoint. Very much like what we did with binary search earlier on. However, I'm doing integer division, which has the effect of rounding down truncating. That means I choose nine as my midpoint. That means 80 becomes the root of this particular subtree. The left subtree is empty and the right subtree contains the 96. You'll notice that we get this biased effect where all these little subtrees at the bottom level go to the right. But that's okay. It's an artifact of the particular strategy that we're using to create the tree and it doesn't affect our balance. This is how we create the tree. As you can see for the 10 items, if you take the log of 10 to the base 2, you get a value that somewhere between 3 and 4. Every single item in the tree can be found with most four comparisons. The height of this tree is indeed log n. Now what happens if we want to insert or delete items from this tree? It's a search tree. The main thing we're going to do is look for things. We want to look for the 22. We've looked for it the same way that we always look for things in trees. 22 is smaller than 25, so we go left. 22 is bigger than 15, so we go right. 22 is bigger than 16, so we go right and this is 22. This is the item that we want, we found it for comparisons. Or we can do the same process like we did with the sort trees for doing an insertion. If we wanted to insert a new item, maybe we wanted to insert 14. 14 is smaller than 25, so we go left. 14 is smaller than 15, so we go left again and 14 is larger than 12, so we go right. 14 would be inserted as the right child of 12. What about deletions? Deletions turn out to be the most complicated maintenance aspect of this kind of tree. Let's consider the deletion. The first one up is that we're going to delete the leaf 43. As you might expect, we'll just go ahead and delete it. That one's pretty easy. It doesn't have any children. The hole that we create is at the bottom of the tree. It's minimally disruptive to our balance, we're good. That one's an easy case. We're going to put the 43 back and then we're going to delete the 42. Imagine the 43 is back, and then think about how you would delete the 42. Stop the video for a minute and draw the tree the way you think it would look if it contained a 42 were deleted. Ready? Here's the slide showing 42 deleted. The tree looks almost identical to the last slide. What happened? We deleted the 42 from where it was located and that created a hole. We filled the hole by promoting its child, 43. That has the advantage of pushing the hole that's created by the deletion down to the very bottom most level of the tree. That's a good thing because it minimizes the disruption to the balance. Because 42 only had one child, who is very easy to figure out what we should promote. The next example, that's going to be a little bit harder because we're going to delete 70 and 70 has two children. Let's restore the tree, put 42 back where it belongs and think about how we wanted to delete 70. Stop the video a minutes and draw the tree the way you think it might look if we delete its 70. Ready to continue. We deleted 70 [NOISE] and it looks like we've promoted 80 up into its place. Then that created a hole where the 80 used to be. We promoted the 96 up to replace the 80. Replacing the 80 with the 96 was easy. Deleting the 96 was easy because we've already covered those two cases. Again, we've minimized the disruption to the balance by pushing the hole created by the deletion as far down as possible on the tree. But how on earth did we decide that 80 was the correct item to replace the 70 that we deleted? We could have just as readily replaced it with 42. I think if you try that, you'll find it works just as well. This example isn't as helpful as it could be. Because it doesn't really clarify why we are doing what we're doing. Let's try another example. In the final deletion example, we're going to delete the 25, we'll put the 70 back, restore the 80 and 96 to the original positions. Now we're going to delete the 25. Stop the tape for a minute and draw the tree the way you think that it should look if we properly delete a 25. Ready? >> We would choose to replace the 25 with the 42. You may find this an interesting choice. We did not replace it with the 15 or the 70, either one of its children. We'd replace 70 with one of it's children. We didn't replace 42 with one of it's children. Are we being inconsistent? It might seem that way at first, but it turns out we're really not being inconsistent. Let's finish the details of the deletion first. When we delete, the 25 and replace it with the 42, that creates a hole where the 42 used to be, so we will promote the 43 like we did the last time, and that creates hole down at the bottom. The hole created by the deletion is minimized. That's a good thing, we've accomplished that goal. Let's check ourselves and verify that if we do our in-order traversal, we get sorted lists that we used to get it. All well and good so far. Why on earth would we pick the 42? What possible advantage does it have? Stop a minute and see if you can figure out, why we pick the 42. If you're ready to continue, we're picking the 42, because it's the inorder successor. If you think about it, that makes sense because we want to preserve, the inorder traversal of the tree. What that means is we could just as readily have picked 22, which is the inorder predecessor. This is the inorder, predecessor, and 42 is the inorder successor. Either one will work perfectly fine. However, in the interests of reproducibility and consistency, we're always going to choose to use the inorder successor. What you might want to do, is go back and look at the deletion for 70 again, and you'll see that we actually picked 80 because it's the inorder successor of 70. It turns out that the inorder successor, of an element will always be a type one or type two deletion. We'll have no children or one child, it can never have two children. Because if it had two children, it' s left child would be the inorder successor instead. That takes care of the insertion and deletions that go with this simple tree. We search for individual items, we do our inorder traversal, we get the sorted list if we want to work with the entire dataset. Our insertions are very simple, our deletions are pretty simple, they attempt to minimize the disruption to the balance by pushing the hole to the bottom. However, over time, this tree will get more straggly and more general. What do we do? We do our inorder traversal, we get our sorted list, and we start over, we rebuild the tree. If our tree is large, this could be a time consuming activity, but we simply schedule it [NOISE] for a point when that doesn't matter. We do it overnight, we do it on a holiday weekend. We can either do this when our performance degrades or we can simply schedule it to happen on a regular basis; once a week, once a month, depending on how many changes we usually make to our data structure. There's a summary. We're going to move on to the other two examples. The one we're only going to spend a little bit of time on is the AVL tree, the height balanced tree, and then we'll also talk about B-trees, and we'll spend a little bit more time on them. They are for a higher order. [NOISE] AVL trees are also called height balanced trees, and the AVL stands for Adelson and Velsky and somebody else, the three people that came up with them. The point of a height balanced tree is that it has a very strict definition of what it means by balanced, that the difference in the height of the two sub trees must be less than or equal to one. With B-trees, you will notice that we also have a fairly strict set of rules that we have to follow. It's a completely different set of rules. But what the two approaches have in common, is that they require you to strictly adhere to the rules every time you do something to the tree, you have to spend effort checking and fixing if needed. Red-black trees, splay trees, 2-3 trees, 2-3-4 trees, are simply variations of B-trees, and you will see B-trees discussed under those names depending on the books you are using. On this slide I've given you some references for AVL trees, you can find them in Horowitz and Sahni data structures book, as well as some others, but they have a nice comprehensive coverage. I think the best reference for B-trees is the Files and Databases book by Smith and G. M Barnes, but you will see B-trees best in most standard tax, or you may see with B-trees are minor variations [NOISE] in the rules, from one book to another. [NOISE] briefly, what I'd like to do is a simple example of an AVL tree. >> Suppose I have a very simple tree with three nodes containing up on it. In addition to the data field of the node, I'm going to maintain a balance factor. The balance factor for any node that is leaf is going to be zero because the left and right subtrees and both cases are empty and zero minus zero is zero. The balanced factor for D is also going to be zero, because the height of the left subtree is one, the height of the right subtree is one, and the difference between one and one is zero. This is clearly a height balanced tree. If I wish to add a node to this tree, for instance, I could add an F, which as a leaf would have a balanced factor of zero. If you haven't noticed already, you'll notice I'm counting the height of the tree a little bit differently. I'm counting vertices instead of edges, because it's just simpler for this approach, for this application. There's my balance factor of zero for my new node, but I have to adjust the balance factors for the rest of the tree. The balance factor for G becomes one. The balance factor for B remains the same. The balance factor for D becomes a minus one. Now, even though the definition for height balanced whose absolute value signs in practice, you retain the sign on the balance factor, because that helps you to distinguish if the left subtree is bigger positive or the right subtree is bigger negative. Just something that makes life a little bit simpler. We're good so far. If I wanted to add additional nodes to this tree, I could add nodes any of the places where I just put it in dotted lines without seriously affecting the balance of the tree. But suppose I wanted to put in an E. Well, in order to maintain the in-order traversal of the tree and maintain this properly as a search tree. There's only one place I can put E and that's as a left child of F. And E would have a balanced factor of zero. The balance factor for F becomes one. The balance factor for G becomes two. If I corrected it at this point, the balance factor for D would also be two. G represents the point of unbalance. Starting at the point of unbalance, the place where the problem is occurring, if you will, is in the left subtree of the left child of the point of unbalance. This is going to require something called an LL rotation. The problem is occurring here, which is in the left child of the left child of point of unbalance. Well, G, what are we going to do about this?. This looks pretty messy. But the term rotation implies that we're going to somehow rearrange things. That's exactly what we're going to do. We're going to rearrange things, but we're going to do it in a way that maintains the search tree characteristic. I'll put zeros balanced factor in there because that part is going to be unaffected. If I rearrange my tree like that, I'm sort of rotating F up here and G down here. Hence the rotation term. The balance factors for E and G both become zero. The balance factor for F becomes zero. The balance factor for D becomes minus one. my tree is back in balance. If my tree had originally looked like this, and instead of inserting an F, an E, I inserted the F back. That would've gone as the right child of E. I would have had the same unbalanced at G and I would have needed to have done something called NLR rotation, to fix this particular situation. The net effect, as it turns out, would have been exactly the same. I would have rotated F up to replace G and make G, the right child. I would have gotten the same result with that rotation as I did with this rotation. If you do the in-order traversal of the tree that we ended up with, you get B D E F G, which does give us the sorted list. The search tree characteristic has been preserved. If you're interested more in AVL trees, take a look at the Horowitz and Sahni book. It turns out there are four types of rotations, LL and LR, which we mentioned, there are also RL and RR. Some of these have a couple of sub cases. Horowitz and Sahni give you all of the code, they give you numerous examples. Feel free to take a look at that if you're interested in AVL trees. I expect you to know that they exist, to know what the definition is, to perhaps be able to do a really simple example like the ones that we've done here, and to have an appreciation of when they might be relevant. If an application comes up where you want to consider them, you know enough to consider them and go do some more research. Now we're going to finish up our discussion of such trees by talking about B-trees, and B-trees apply to higher-order trees. That is why we use the term m-ary. Notice there are several rules that apply to B-trees. Most important one is that all of the leaves are required to be at the same level. It's completely a different mechanism for enforcing balance, but it works very well. The other strategy that it uses is that it requires a minimum number of children. >> By having a rule that enforces a minimum number of children, it ensures a certain degree of Bushiness to the tree. In this case, you can see that the minimum number of children is about half. The room is allowed to be the exception to the minimum child rule, because it's allowed to only have as few as two. There are practical reasons for that, that I hope will become obvious to you. The other important thing that we need to know is because this is a higher-order tree, we need to realize that we will be having multiple child pointers and as a consequence will be having multiple records. If you think about a binary tree that has a data value in it. We look at that data value and use it to decide whether we want to follow the left child pointer or whether we want follow the right child pointer. With a higher-order tree, we number the pointers as shown starting with pointer 0 at the left, pointer 1, pointer 2 and pointer 3, in this case at the right. We need three data records to distinguish between those four pointers. If we're on a node that has four child pointers, then we would look at record 1, if the value we want is smaller than r_1, we follow pointer 0. If the value we want is bigger than r_1, we come over here to r_2 and compare it to r_2. If it's smaller than r_2, we follow pointer 1. If not, we come up here to r_3, compared to r_3. If it's smaller than r_3, we follow pointer 2, otherwise in this case we follow pointer 3. The larger node is convenient because we get to look at more things very quickly. But we have more comparisons to make to determine which path to follow. The binary tree we only had one comparison, with this tree we have more comparisons to make. The number of comparisons is actually one less than the number of pointers in the node. Let's see how this works in practice. We're going to do an example where we make a series of insertions into this particular B-tree. Here we have a B-tree and the first thing that we want to notice is that all of the leaves are at exactly the same level. Now this tree is not regular, this tree is not complete. You'll see that the first node on the last test two entries, whereas its siblings only have one. It's clearly not regular complete. Over on the left, I've summarized some of the characteristics from the previous slide. You can take a few minutes and work out how this tree conforms to that example. Pause the slide and do that and in a minute we'll continue. First of all, we're going to insert the value X into this tree. As always, with a search tree, we start at the root, X is greater than L. We go right, X is greater than R. We move over to the next value, X is greater than V. We follow the right child pointer and we come down to a node that only has W in it. Well, since the order of our tree is three, we can have up to two records in a node. Max records is equal to 2. We have room in this node, so we simply go ahead and add it to the node with the W, is it okay. Clearly we could add values to the node containing G and the node containing J, and the node containing L just as easily. The next thing we want to insert is the value P. by starting at the root and working our way down, we determined that it goes into the node that already contains MN. Since the limit on records is 2 this node is 4. What are we going to do? Temporarily, we're going to shut in there and create a temporary over sized node because this is where we want to go, you need to put it in the right place to deal with the consequences. The consequences is going to be that we split the node. We split the node basically by making it into a little tiny tree. Since the little tiny tree needs to preserve the such tree characteristic, the middle value add comes parent. Well, we incorporate that into our actual tree that has the effect of shoving N up to the real parent RV. That's now too big because RV doesn't have any room for that. We do the same thing and we do another split and this time R goes up to the parent and that looks like that. R goes up to the parent, there's room in the parent, we're good. The notice that the effect of this split was to create room in the tray. The node M, the node P, the node N, and the node V all have room to have values inserted where they didn't before. [NOISE] The next thing we want to insert is the node containing D. D is smaller than L, we go left. D is smaller than E, we go left and the node containing AB is full so we do as a temporary node, and then we'll do a split. B goes up to the parent like it did before. Again, that's too big, so we do a splits. E goes up to the parent like it did before, that's too big, we have to do a splits. [NOISE] I suggest that this time when we do a split, because we were at the root of the tray, we effectively add a new level to the tree. B-trees grow and as you'll see in the next example shrink by at the level of the root. This is the primary reason why we allow the route to only have two children if necessary. Now that we've done all of these insertions and had all these splits, we've actually created a lot of space for subsequent insertions. We won't necessarily need to do any splits for awhile. There's a lot of work associated with splits but they also create new space and reduce the need for future splits, it averages out. Notice that the height of the tree is increased by one over here on the left. Now we're going to look at a series of examples involving deletion. This is a different tree and point of fact, this tree is an order 5 tree. It has a different set of requirements for the minimum number of children and the maximum number of children, the minimum number of records and the maximum number of records. I don't know what the B is all about, but the height of this tree happens to be 2. We're going to start by deleting R. We go to the root, R is bigger than N, we go right, R is bigger than Q. We go to U, R is smaller than U, so we follow the pointer that's in between them. We arrive at the node containing RST and R is the item that we want to delete. Over on the left, we see that while we are allowed to have up to four records in a node, the minimum is only two. Since there are actually three of this particular record, we could easily just delete RS and T and we're done. It looks like that when we're finished. That was easy. Next we want to delete H. Well, we work our way down from the root to CFI, from CFI down to GH. We can't do what we just did, we can't just delete it because there are only two records in this particular node. Having only one record would cause a problem and go below the minimum threshold. That creates a node that's too small. Stop the slide for a minute, stop the video and think about how you might deal with this particular problem. Maybe write down two possibilities of how you could work with this particular problem and address the deletion. >>Well, there's more than one way that you can choose to deal with the deletion but the way we are going to deal with it is we are going to consolidate with a sibling. If you thought of that strategy good for you. In this particular instance, we are going to consolidate with the next adjacent sibling, KL. Here we're showing what happens if we delete H, its too small. In order to consolidate with sibling KL, we also have to demote the value I from the parent because when we get rid this child, we get rid of the pointer to that child, which means we have to get rid of a record in the parent because we don't want to have records with no child pointer in between them. But we have room in our new consolidated node for I because we're allowed to have up to two records. Some of the issues that you might want to think about are, why did we consolidate with sibling on the right KL, would it have worked just as easily if we had consolidated with the sibling on the left? It should have, in point of fact, this then becomes an exercise in logic and efficiency. Do you want to always start with a sibling on the right or do you want to look at both of them, and then decide which one's better? After all, whichever one you start with, sometimes it will be full, and you won't be able to consolidate, and so you may have to try the other one anyways. Give some thought to which one you try first, and does it make any difference. Now some of you may have suggested that instead of consolidating with a sibling, that you borrow from a sibling. Let me go back a minute, and as you'll see here, we could not have borrowed from a sibling in this case. But take a look at the node containing ST, and its sibling VWX. Well, if I had wanted to delete the T, then I could possibly have solved my size problem by pulling U down from the parent, and then I could have pushed V up to the parent, and so I'm basically borrowing from a sibling. That would have had the effect of U, and V, and then it would have been deleted here. That type of shifting or borrowing is another reasonable strategy that you sometimes see associated with B trees. However, strategy that we just discussed of consolidating with a sibling will be what we consider to be the primary strategy. The next item that we want to delete is B. Well, the same thing is going to happen with B, that happened with H just now. The node that is left behind A, is too small so we're going to consolidate with sibling DE, and we'll demote from the parent just like we did before but when we do that, the parent then becomes too small. We'll do exactly the same thing that we did before. We repeat this process, we consolidate with the sibling, which is QU in this case, demote from the parent that turns out to make the parent empty. Well, that's not a big deal because that's the root of the tree so we simply chop it off, and now we have a tree of height one so we've updated that. The deletion of B is complete, and you see that B trees grow, and shrink. At the level root, the leaves are always the same. Their efficient retrieval times come from the fact that they are very short and very bushy. They contain a lot of nodes because they're higher-order, and they're very short because of the higher order. That allows a fairly short path for the root to a particular node. The trade-off you're making is that with any node you have to make more comparisons to determine which child path to follow. But nonetheless, they can be an efficient structure. Historically, the motivation for B tree stems from the fact that if you access things off of hard drive, accessing the actual drive itself can be a lot more time consuming than anything you do in internal memory. By having records that contain more items, you can read off more entries at the same time, and say reduce the number of times that you go out to the hard drive, and look for something. That's the traditional motivation, the other thing you might want to know about B trees is there are standard variations of B trees. B star trees, B plus trees, B star trees, for instance, are very similar to the B trees that we've just looked at but they have a different bushiness requirement. Instead of being about half, it's about two thirds, and what you'll find with those is that there's more of the borrowing going on that we ventured as an alternative earlier. That seems to work a little bit better with those slightly fuller trees. Just be aware that there are lots of possible variations out there on B trees. I will only hold you responsible for the specific variation that we've discussed here [NOISE] in the material. Next, we will talk about the search strategies that do not depend on the data being sorted but like before, not being sorted is not necessarily not being organized.