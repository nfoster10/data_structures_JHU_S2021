  PROFESSOR: Welcome to the module on sorting. Sorting is simply a way to organize data. It's not the only way to organize data. But it's obvious. And it's common. It facilitates retrieval, because it allows you to figure out where the item will be in the file whether or not the item's actually present. However, whether or not you need to do sorting is really going to be a function of how you're going to use the data. If you need to access your data and a sorted manner, then you need to do some sorting first. So the real question is to think about how you're going to use your data-- what types of accesses you're going to be to do. In some cases, you may not need your data to be sorted all. So this won't be an issue that you spend time on. We'll come back to this question when we get to the module on searching after this. For now, we're just going to look at some basic sorting strategies, try to match them to the various problems, and look at the situations in which they're good and the situations in which they're less than desirable. It is important to be able to match the sort to the problem. Sorts are not like greeting cards. There's no one size fits all or all occasion sort. By way of illustration, let's keep in mind as we go through this material. An organization, perhaps, is located in Washington DC area. As such this organization may have members from Maryland, Virginia, and Washington itself. Sometimes you might want activities, like updating a particular record to reflect the fact that person paid their dues or to reflect that maybe they've changed their address or they've become active in formal role of the organization, whatever the case might be. Other times, maybe you wish you send out geographically oriented mailings to your membership to notify them of activities. In that case, it might make sense to sort by zip code. Whereas, for accessing individual records, maybe you really want to be sorted by Social Security number. Two different ways to access same data. A third way to access this data might be to recognize that you want to have a directory of all your members. And so once a year, you do some sorting by first name, last name combination and create a directory.  Lots of applications are like this where you need to use your data in different ways for different purposes. And those are the issues that you need to think about when you decide how you want to organize it and, ultimately, whether or not you need to sort it.  Here's a simple example showing a small inventory of wood products. And we wish to organize them by the quantity that we have on hand. So up at the top you see random, unorganized data. And down at the bottom you can see that it has been sorted, or more to the point, organized by quantity on hand. This is a reasonable strategy in some situations, because it allows us to only look at the beginning of the list and determine which items we might need to reorder.  Let's get some basic terminology out of the way. When we talk about sorting, we will use the term "record" to mean a unit or a cohesive piece of data. So this might be a personnel record consisting of a name, address, telephone number, the benefits this person has signed up for, their dependents are, what their salary is, what their start date with the company is.  It might be really easy to envision that this record is not the same size for every person. Some people have more dependents than others, for instance. A file is a collection of unit records.  And if we have n records, we talk about the size of the file being n. Records may be fixed in size or variable in size. It's generally much easier to deal with ones that are all the same size. And if the size variation is small, it can be the easiest to deal with that variation simply by padding the smaller ones to make them all the same length. However, most of the techniques will work equally well with variable size records, which you add in a little bit of code to deal with the variation in size. So for the most part, we will ignore that factor. The key is the part of the record that you look at to determine if it's the record you're looking for. So it's the search key, if you will.  We haven't talked about searching yet. But we're organizing our records for the purposes of retrieval. And that's what we look at-- to see if we have obtained the record we desire. Keys come in two general categories. And this the kind of thing you'll look at more detail if you take the follow up course 605-441, principles of databases. A primary key is generally required to be unique and might be something like a social security number or a stock number. Whereas a secondary key is not particularly unique. And in the example I gave you before when we were sorting by zip code, we're sorting by first name, last name combinations, those would be examples of secondary keys. Whereas the social security number we used in that example was a primary key. An external key is when the key is external to the record.  Now there are two ways of doing that. It can be duplicated or not. If it's duplicated external to the record, then you have to be concerned to maintain everything identically and not have integrity issues. So that can be a little bit more maintenance. Sometimes you'll see the external key maintained completely outside the record with no duplication in the record, no Integrity problems. But if they become separated, that can be a very serious issue. We'll do a simple example with an external key coming up. The other issue that we need to cover is to talk about external versus internal sorting. Internal sorting is what we do when the amount of items to be sorted is small enough that we can bring it all into main memory at one time. We bring it in. We do our sort. We write it out to however we're storing it, whether that's in a flash drive or a hard drive or whatever the case may be.  We will primarily be discussing internal sorting techniques. External sorting techniques are for files that are too big to be brought into main memory all at once. So you bring them in pieces. Do one of the standard internal sorts on each piece. Then you have to find some way to consolidate them. Usually the way you consolidate them is some form of merging. We will be talking about simple merging as part of our internal sorting strategy sequence. And that will give you a good jumping off point for doing external sorting, if the need arises.  Here's a simple example of sorting by address. And in this case, what we're really tough talking about is an external pointer to the record. Now, this isn't really a very good key, because it's simply referring to the position-- so the first record, the second record, the third record. And so what you really want to envision here is that the first record is beach. And the second record is oak. And the third record is birch, and so on and so forth. And we wish to sort our records by quantity on hand. And the least quantity on hand is maple, which is in location 6. So our table of pointers has been sorted by that quantity. And this is maple.  And 4 is birch.  Sorry-- 4 is actually pine.  5 is walnut, and so on and so forth. So we would have an external reference that points to the position that the item is located.  And we sort those values. And this is a nice technique, because it allows us to rearrange our data without moving the records themselves. And this can be convenient when the records are particularly large.  Most of the techniques that we're discussing work perfectly well with external pointers or not. So I would assume that you'd use them when when they're called for.  And we won't be particularly discussing them.  When you're trying to select a sort for a particular application, there are a lot of points to consider. This list is a good start. But it's not necessarily exhaustive. You need to think about the size of the data. Is there any order imposed on the data? Sometimes data is mostly sorted already and what you're really doing is verifying. Sometimes it's in reverse order. Sometimes it's very random. Is it this order all the time? You may not have any control over the way the data arrives. It may come from another source. It may be a function of inquiries from a customer base. So that's something to take into consideration, because some sorts respond to order and others do not. But they don't all respond to the same order. Is there a distribution on the data? This is a tricky one. A lot of times, we simply don't know if there's an order on the data. And when we don't know, we have a tendency to act like the distribution is uniform. It may or it may not be, and it's important to realize when you're making that assumption and whether or not is a valid assumption. What do I mean by distribution on the data? I'm talking about how frequently each individual record is accessed. A uniform distribution would be that each record is equally likely to be accessed. But that's not really true. There are a lot of situations where some records are more likely to be accessed than others. For instance, if you were to run a pharmacy, you probably have people that come in once and get a prescription. And you never see them again, because they only get something when they have the flu or a bad cold. Other people you may fill prescriptions for every single month, because they have a chronic condition. The person who seldom gets prescriptions, they start getting more if they develop a chronic condition. The person who already gets lots of prescriptions may get better, or they may move and stop getting prescriptions from you all together. So even when you have a distribution, and you know what it is, that distribution can change.  Is there code that you can reuse? How much programmer time do you have to put into it? How quickly does it need to be done? How often do you have to do to sorting? If you don't have to do the sorting very often, it's not especially important that it be very efficient. How is your data going to be used? Are you going to be accessing it in lots of different ways, where you frequently need to resort it by different keys. Or maybe you only sort of once in awhile.  What is the efficiency of the various sorts that are available? How much space do you have? Some sorts work in place and don't really require any extra space. Other can require a significant amount of extra space.  The various sorts tend to fall into categories that are shown in this slide-- exchange sort, selection sorts, insertion sort, merge sorts, and radix sorts. And we'll look at some examples in each category. Sorts tend to range in cost from linear up to cubic. Where probably what we'll look at will just range from linear up to quadratic.  Most of the time, however, you're really just going to see times from n log n up to quadratic. And for the most part, linear behavior tends to be a special case.  In the section of the website for this module, you will see a file that will allow you to print this chart that's shown on this slide. I would strongly encourage you to print the chart and to fill it out as we go along and keep track of the various sorts. It will provide a nice way to do a comparison of all the different sorts. And I think it will allow you to get a better grip on what the choices that are out there. For best, worst, and average, I mean the performance in the best, worse, and average case. So you're going to be putting big O values in those boxes. And in addition to the big O value, you should be labeling it with the kind of data that gives that performance. So maybe for one sort, in order date gives the best performance. But for a different sort, random data might give the best performance-- and so on for each of the three columns. Sensitivity to order means does the sort respond to order in the data. Some sorts do. Some sorts don't. Some sorts perform the same regardless of what kind of data you throw out. Some have significant differences. For the purposes of this course, sensitivity to the order data means that it changes the magnitude of the complexity. It goes from linear to n log n, or it goes from quadratic to n log n, or it goes from quadratic to linear. If it doesn't change the magnitude, we don't consider it to be sensitive. It doesn't mean that there's no change in the performance. It's just not a big enough change to reflect the big O magnitude. Are there space requirements beyond the space to hold the actual data? And does this sort require any particular implementation? That's what's reflected in the last column. Some have to be done an array. Some would work better in a linked list. And some will do either.  The first category is exchange sorts. And we'll be looking at bubble sort and quick sort. Then we'll look at simple selection sort, a couple of tree sorts, in particular binary tree sort and heapsort, followed by a quadratic selection sort. We're going to look at simple insertion sort. And then we're going to look at shell sort as a way to leverage the advantages of a simple insertion sort. Address calculation will be deferred until after the material on hashing. Merge sorts, as I mentioned before, are the jumping off point for all of the external sorting techniques. And we're going to look at a simple merge here that works well also as internal sort of technique, as well as the variation natural merge. So let's start with our exchange sorts and go old bubble sort. Bubble sort is likely a sort that you've seen before. It's often used as an illustration in introductory classes. It is simple, but not very efficient. It tends to be good for small data files, because it needs to be done in an array. So you have to be able to allocate enough space to do it. It's good for what we call a throwaway application. All right, what do we mean by a throwaway application? That's one in which we don't intend to do it very often or just for a short period of time and then we're done with it. We're essentially going to discard it. So under those circumstances, it's not especially important that it be efficient.  So we'll say this is where efficiency isn't not important.  Bubble sort darts with a simple double nested for-loop. And they are a couple of standard, easy modifications we'll make to tweak up the efficiency just a little bit.  For illustration purposes, we have a small file with n items in it. And you can see, it's fairly random. And over to the left, we will keep track of the number of comparisons that we make, the number of exchanges that we make, using the columns labeled C and E. Bubble sort uses a series of pairwise comparisons through the file. So we start by comparing 25 and 57. And they are ordered with respect to each other. So we don't do anything. 57 and 48, however, are out of order with respect to each other. So we swap them. And you can see version reflecting the end of the pass at 25 is in the same place, but 48 and 57 have been switched. 57 would be here. We compare it with 37, and we switch again. 57 and 12, one more swap. 57 and 92, no swap there. So the 57 ends up right here. 92 and 86, we swap again. 92 and 33, one final swap, and the 92 ends up at the end of the file. So we've kind of picked up the largest item that we've seen so far. And bubbled it further up the file-- hence, the name. So for that pass we did a total of seven comparisons and five exchanges.  And then we repeat the process. 25 compared to 48, no problem. 48 and 37, that results in a swamp. 48 and 12, another swap. 48 and 57, that's OK. So the 48 stays there. 57 and 86, that's OK. 33 and 86 results in a swap. 86 and 92, no change there. Again, seven comparisons, got only three exchanges.  25 and 12-- I'm sorry, 25 and 37, no change. 37 and 12, that's exchange.  37 and 48, no change. 37 stays there. 48 and 57, 48 stays there. 57 and 33, that's an exchange. 57 and 86, no further exchange. 86 and 92, no further exchange. At this point, you might be wondering why we keep looking at the 92. It's not going to move. It's exactly where it's supposed to be. That is a point, so we don't want to keep looking at it, because it doesn't accomplish anything. So now what are we going to do? Well, we recognize that fact.  And if we start over, the little black line is intended to recognize the 92 has been put in its final position, and we don't need to look at anymore. After the second pass, the 86 is in its final position. And we don't need to look at it anymore. After the third pass, the 57 is in its final position. And we don't need to look at it anymore. What that allows us to do is to do reduce the number of comparisons that we make. So instead of making-- after all, those were comparisons that didn't result in further exchanges. So instead of making seven every time, we reduce the number. And we get 7, 6, 5, 4, 3, 2, 1.  And as you can see, we continue to make our comparisons and our exchanges on each pass until we have made a total of seven passes. Feel free to stop the video and goes through the passes and make sure that they make sense to you. OK, so that was one change that we can make to our simple double nested for-loop. We can drop that last item off. So it doesn't keep it from being a double nested for-loop. But it does help a little bit. There's one more change that we can make that might be advantageous. Stop the video for a moment and see if you can figure out or observe something that might be useful to us.  OK, well perhaps you observed that in the second to last pass we did not make any exchanges.  That means that everything is ordered with respect to each other. So we're really done. So I put a dotted line to show that we were really done after that point. And we could have stopped there. So that's the second change that we will make. We test a flag see how many exchanges were made or count the number of exchanges. And if we didn't make any, we're done. Of course, at this point, if we skip the last pass, we're skipping one comparison. Big deal. But if we're able to stop much earlier in the process, that could be a considerable advantage. So let's take the cost of bubble sort.  We have a simple summation that represents that sequence of comparison-- seven six, five, four, three-- so we can write that as that summation. Well, this is a standard closed form summation. And it's worth your while to memorize it if you haven't seen it before. The closed form solution looks like that. And we'll probably use this again.  OK, so that's the cost of bubble sort if you count the comparisons.  It's clearly quadratic.  Now, sometimes when you do a comparison, you also do an exchange. But you don't do an exchange every time. However, in the worst possible case, you would do an exchange every time.  All right, well, in that case, you have the same summation. But you have a factor of 2. Well, the rules of working with summation allow us to pull that constant 2 out. So we get 2 times n times n plus 1 over 2. The 2's cancel. And we get x squared plus n. So it's still quadratic. But we have different constants and lower order terms. Let's look at an example of a file that would represent a worst case situation for bubble sort.  A reverse ordered file-- and I would encourage you to take a few minutes and stop the video, work this example out. We'll do every single comparison and every single exchange. So that's going to be like the n squared plus that we just looked at.  A final looks like this-- we will make one pass. Compare the 1 and the 2, the 2 and the 3, the 3 and the 4, the 4 and the 5. We will make no exchanges, so we will stop after one pass.  There's our early out. The number of comparisons is n minus 1.  The number of exchanges is zero. So we're done. We get our early out. So bubble sort gives you linear performance when you have an in order file.  Clearly, bubble sort needs to be done in an array.  And you should have enough information to fill out the cost and also see that bubble sort is sensitive to the order of the data.  Now let's move on to the other example of exchange sort, which is a quick sort. A quick sort is actually as close as it comes to the all occasion sort. It's quite efficient. It is written in a recursive manner. Although you do find iterative versions available.  However, the strategy is inherently recursive. So we'll look at it that way. You have a couple of things you need to do. You need to select a pivot. You need to do something called partitioning. And then you recursively repeat that process until you get a sorted result. So that's where your recursion comes in. Any sort that uses this basic strategy is considered a quick sort. And there are lots of ways to do the partitioning. We will look at a specific one. And there are lots of ways to select the pivot. And we'll talk about that a little bit as well. Be careful when you look at some examples of quick sort in various textbooks. They tend to make it look like you figure out where the pivot goes, and you slide everything over to make room. And that's not the case at all. So be careful.  Here's a simple partitioning algorithm. Almost every single book seems to present a slightly different partitioning algorithm for doing quick sort. I think this one is reasonably efficient. And it minimizes some of the comparisons. This is the one I would like you to use for the purposes of this course. First, you select a pivot, make a copy of it. And we'll come back to the pivot selection later. We're going to use two pointers. We're going to use the down pointer, which goes to the beginning of the piece that you're going to do the quick sort on, and the up pointer, which goes to the far end of the piece on which you're doing the quick sort. And the object is to jockey the two pointers towards each other. And when they meet, that's where the pivot belongs. So we start with the up pointer, which is at the right-hand end. And we move it to the left. Objects at the right-hand end are supposed to be bigger than the pivot.  So we want to move the pointer to the left until we find something that's smaller than the pivot. Then we copy that item on top of the down pointer. Well, that doesn't hurt anything, because the down pointer is where the pivot was in the first position. And we already made a copy of that. So we should make a note that this approach assumes the pivot is in the first position of the partition.  And if it's not, it's easy to swap it to that location and then start this procedure.  Then, we start with to down pointer and move it to the right until we find something that's larger than the pivot. At the left end of file, items are supposed to be smaller than the pivot. So as long as we're finding things that are smaller than the pivot, we're good. And we keep on going until we find something that's bigger. That value gets copied on top of the up pointer. And we go back and forth until they run into each other. And that's where the pivot belongs. I'm going to illustrate it with an example using the same data set that we used for bubble sort.  So what you see now is the selection of the first item as our pivot. We've copied that off to the side-- over here. We have our up pointer and our down pointer at the respective ends of the file. And now, we're going to start with the up pointer. This is at the end of the file where things are supposed to be larger than the pivot. And since 33, 86, and 92 are all larger than 25, we keep moving the up pointer to the left. And eventually, we hit the 12. And that's where it stops. And we overwrite the 12 on top of the down location. Again, since we've made a copy of the 25, we can afford to do that. We now pick up the down pointer. This is the end of the file where things are supposed to be smaller than the pivot. 12 is smaller than the pivot. We move the down pointer to the 57. That is bigger than the pivot. It is out of place. So we copy the 57 on top of all of the 12. We keep going. We move the up pointer. 37 is larger than 25. 48 is larger than 25. We run into the down pointer on the 57. And that is the location of the pivot. We are finished with our first pass. And we've created two partitions-- one of size one, which is trivially sorted, and another partition, which has six items in it. We now continue and work on that partition with six items. We will select 48 as the pivot. We have our up and down pointers set up. We start with the pointer. 33 is smaller than 48. So right away we start swapping. We pick up the down pointer. Well, 33 is smaller than the pivot, obviously. But so is 37. We get up to 57. And then we copy the 57 on top of the up pointer. We pick up the up pointer. We look at 86. We look at 92. We run into the down pointer on the 57. And that's where the value 48 goes. We're left with two partitions. And we keep going. Well, we'll start with the partition containing 33 and 37, which turns out that partition's actually sorted. But we don't know yet. We pick 33 as our pivot. We set up our pointers. We start with the up pointer. 37 is larger than the pivot. We move it right away. We run into the down pointer. And we're done. We're able to put the pivot in the correct place. We're left with a partition of size one, which is trivially sorted-- the 37. And we have one more partition left to sort. And we pick the 92 as the pivot. We set up the up and down pointers. We start with the 57. That's already smaller than the pivot. So we swap it. We pick up to down pointer. 57 is smaller than 92. 86 is smaller than 92. The down pointer runs into the up pointer. And our pivot goes in that location. One more pass-- we have to pick 57 as the pivot and sort out to 57 in the 86.  And that takes care of that. So that's the end of our quick sort.  How many comparisons did we make-- start over.  How many comparisons did we make on our quick sort?  Well, if you were to go back, and you were to count up all of the comparisons and the exchanges, you should find that we did 26 comparisons, which is roughly three times the number of data items. And we did eight exchanges which is exactly n in this particular set of data. And this was a fairly random set of data.  OK, so I'm all done with my quick sort. And that's the final result showing all the partitions consolidated. All right, now, let's take a look at what happens if we do our quick sort on data that is actually sorted. This was a fairly random example.  Suppose we have a small file.  Well, if we do an entirely ordered file, it turns out that we require 21 comparisons. And for the reverse ordered file, it's going to require 24 comparisons. Well, for the specific example that we just do it, this is actually 26 comparisons and eight exchanges. And I need to point out to you that the example we did at the slides was n equal 8. And this only n equal 6. So what you might want to do for your own edification is to take 1 through 8 in sorted order and reverse order and count the number of comparisons and exchanges that you do in both of those cases. And for this example, n is 6. So n minus 1 is 5. And you're looking at 5 times 6 over 2 as the cost of that summation, which is 15. So clearly, this is a little bit better than quadratic behavior. And we can attribute that to the constants and lower order terms perhaps. And you'll see the same thing with the reverse ordered data. So what's the overall cost of quick sort?  What happens when you do a quick sort is you have a file, and you partition it into three pieces-- things that are smaller than the pivot-- and of course, they could be equal-- things that are bigger than the pivot, and the pivot itself. And we are going to suppose, for the sake of argument, that n is equal to a power of 2, just because that makes life a lot simpler. So the first question we have to ask ourselves is what is the cost to partition.  All right, we have a partition of a particular size. And we want to split into two partitions. And the cost of doing that requires us basically to compare the pivot to every single element in the partition. We jump back and forth. But we only look at each element once. And we ask the question is it bigger or smaller than the pivot. And then we decide what to do with that as a consequence of that comparison. So basically, the cost to do the partitioning process is linear in terms of the size of the partition.  Now, what you might want to notice is that if the size of the files 2 to the k, the size of this piece is approximately 2 to the k minus 1. And the size of this piece is approximately 2 to the k minus 1.  OK, when we do a quick sort, as we go through, we partition our pieces. And we're going to divide it roughly in half whenever we get a good partition.  So we start with a big piece. And we divide it into two pieces that are roughly similar in size.  And we keep going until all of our pieces get down to size 1.  So every time we make the transition from a big piece to pieces half its size, what's the cost of doing that? The cost of doing that is linear in terms of the pieces that we're going to handle. Well, every piece always gets broken down. So you could actually consider this as being linear every single time when you added up all the way across.  Now, this is a considerable simplification. But it's not an unreasonable way to look at. So if you're expending linear cost every time you move down a level, how many times do you move down a level? Well, that's where our assumption about n becomes convenient, because if we break it all the way down until we get to pieces of size 1, that means that the number of levels or the number of times we change levels is k, or log n, which in a very simplistic way tells us that this should be an n log n cost. That, in point of fact, turns out to be exactly what it is. If you wanted to add this up in a little bit more sophisticated manner, you could count the number of pieces that you have. You have one piece of size 2 to the k here. You have two pieces of size 2 to the k minus 1. You have four pieces of size 2 to the k minus 3.  I'm sorry, I got my exponents mixed up-- so on and so forth, down to n pieces of size 2 to the 0. So you could actually create a summation that represented all of that.  Do a little bit of simplification.  Because remember, 2 to the k equals n. n is a constant with respect to the index. And we can pull that out.  So we get n times the number of times that we go through the summation, which is k times. And k is n log n. I'm sorry k is log n. And there's our n log n. So if the partitions are very unbalanced, and if you work through the example with the ordered files in the previous slide, you'll see that. Then your performance is quadratic. For those of you that continue on to the algorithms class, you should in that course do a little bit more formal proof of the actual cost of quick sort. Quick sort clearly needs to be done in an array. So it can be done with random access data.  And you should have enough information to fill in the rest of the chart.  All right, now, let's take a few minutes and talk about pivot selection for quick sort. The easiest way to pick a pivot is to pick an arbitrary location. In the example that we did, we selected the first item. You could pick the last item, the 17th item, the middle item, the 24th item, any item that you wanted. Picking an arbitrary element gives an OK pivot but not a great. pivot. And what I mean by that is that sometimes it's going to be really good. And it's going to give you two reasonably even partitions. And other times, it's going to be really sticky. And it's going to give you very unbalanced partitions. So it's potluck. And it turns out that as long as you select an arbitrary position-- the same arbitrary positions each time, that's going to be the case. If you want to do that, your best bet is probably to select the middle one, because that does allow you to explain any order that might exist in the file. And it doesn't hurt if there isn't any order in the file.  If you select another item than the first one as the pivot, you simply swap it with the first location. And then you can proceed with the partitioning algorithm that we already discussed. Another strategy for selecting a pivot is to look at the first, middle, and last locations. And of those three, you select the median. Swap it with the first location. And you proceed with the partitioning algorithm that we already discussed. OK, so what does this do for you? This is an extra step. It requires a couple of extra comparisons to figure out which is the median. Picking the first, middle and last is easy, because we're using random access. So a couple of extra comparisons that doesn't seem like a terribly big deal, although it will figure into the constants and lower ordered terms of your actual cost.  What does this buy us? So stop the video for a minute and give this some thought. And see if you can identify what median of first, middle, and last does for us.  So if you thought about it, we'll continue. What picking the median of three does for you is it prevents you from selecting the very largest items the very smallest item as the pivot. So you will never have an empty partition. You could have a partition that only has one thing in it. But you'll never have an empty partition. Well, that doesn't actually seem like it's such a big deal. It turns out that it's a pretty good deal, because by using the median of three, you get n log n performance for all orderings of data. Being able to eliminate that largest or smallest item from being your pivot gets rid of that quadratic performance. So that's actually a pretty reasonable strategy that's worth doing. Mean sort-- mean sort is what happens when we calculate the arithmetic mean, or the average, and use that as the pivot. Well, first of all, the cost of calculating the average is linear just to figure that out, because you have to look at everything in the file.  You calculate the average. You use that as the pivot. And that's nice because it's always the perfect pivot.  However, I bet if you stop the video for a moment and think about it, you will identify some problems with mean sort.  Well, if you're ready to continue, I hope you will have realized that the average, or the mean, is probably not actually a member of the file. You know, if you have the average of 2 and 6, that's 4. 4 may not be a member of the file. If you have the average of 2 and 5, that's 3 and 1/2. And that's not an integer value. It's a floating point value. So the average is always slope. And so the type may be completely wrong. Well, you could truncate it. But even if you truncated it or rounded it up-- 2 and 5, but if you truncate 3 and 1/2 or round it up, you get 3 and 4. Neither of those are in the file either. That doesn't necessarily solve your problem. It turns out that you can fairly easily and reasonably rewrite partitioning process to use an external pivot.  And that's a fairly simple change. And that solves that problem. But you still have this extra cost of a linear term to calculate the average on every single pass. Quick sort is worth doing, because it gives you n log n performance across the board. But if you think about, your intuition will probably tell you that the lower ordered terms are more significant than they are for the median of three. But these are all options that you have to consider if you want to do a quick sort. 