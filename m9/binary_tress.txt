  This is the third portion of the module on binary trees. At this point we're going to look at a tree from a recursive standpoint. So we want to keep in mind that a tree, and we'll draw a generic example over here to the right, is a collection of three things. A root, a left subtree to that root, and a right subtree to that root. So in this instance this tree is composed of a root, a left subtree, and a right subtree. The left subtree in turn has a root, a left subtree, and a right subtree. The right subtree of the original tree has a root, a left subtree which is empty and a right subtree. So it's truly a recursive definition. Keep in mind the left and right subtree's are also trees and therefore are allowed to be empty. It turns out that recursion is a very natural way to deal with binary trees because of their structure. And many of the algorithms associated with using trees are also recursive. At this point we want to look at identifying a methodical way to move through the trees and visit each node. We want to make sure that we do it in an orderly fashion and we don't miss anything and we don't do unnecessary backtracking. What do we mean by visiting? Well that's just a generic term that includes lots of different activities. It could be writing out the contents. It could be performing a calculation, changing or updating the contents. Whatever you need to do for the sake of your application. We'll just cover that all under the generic term. There are six possible combinations of orderings that you can get from the three pieces that compose a tree. You could have the root first, followed by the left subtree and then the right subtree. Or the root could be first, followed by the right subtree and then the left subtree. Similarly we could have the root in the middle position or the root in the last position. It turns out that we consider binary trees to be inherently an ordered construct.  Generally in most applications, the left child comes before the right child. It may be smaller. It may have greater precedence, but that's a fairly typical thing to do. And in the cases where there is no actual order it usually means it doesn't matter which order we visit them in. So visiting the left one first doesn't hurt anything. On that basis we are always going to visit the left child before the right child and so that will allow us to get rid of three of the combinations and simplify our requirements. The three combinations we are left with are called preorder traversal, where the root comes first.  Inorder traversal, where the root is in the middle.  And postorder traversal where the root comes last.  Now we're going to work out some examples to give you some practice.  If I'm doing and inorder traversal of this tree I start at the root E, but I can't visit E because I haven't taken care of the left subtree. So I go left to B. I can't visit B because I have to deal with the left subtree. I go left to A. I cannot visit A because I haven't dealt with the left subtree. I go left, there is no F subtree. I visit A, I check out the right subtree, it's empty. I go back up to B because I'm all done with the left subtree of B. Now I can visit B. I go right, I have to defer D. So I go left to C, I defer C. C has no left child, I visit C. I go to the right, there is the right child. I'm done with the left subtree of D so now I can visit D. D has no right child, I'm all done with D and I go back to B. I'm all done with B. So finally I can go back to E and visit E. Now I have to take care of the right subtree of B. So I go down to F, defer F while I check out its left child. Well that's empty so now I can visit F. Then I go to the right. I defer H, go to the left, I differ G. G has no left child. I visit G, G has no right child. I'm done with G, I go back to H, visit H, go to the right, defer I. I has no left child. I visit I, I has no right child, I'm done with I. I go back up to H, I'm done with H. I go back up to F, I'm done with F. I go back up to E and I'm done with the tree. So that's what we mean by an inorder traversal.  And as you can see, what we worked through matches what it-- the solution that we're looking for.  Now we're going to work through the same example and do a preorder traversal. With the preorder traversal we visit the root first, then the left subtree and then the right subtree. So for E we start at E, we visit E. We go left to B we visit B. We go left to A, we visit A. A has no left child or right child so we're done with A and we go back to B. We need to go to the right so we go down to D, we visit D. We go left, we visit C. C has no left child or right child we're done with C. That takes care of the left child of D. There is no right child of D, so we're done with D. We go back to B. We're done with B. And now we're done with the left child of E. We now try to do the right child of E. So we go to the right to F and we visit F. There is no left child so we go to the right and we visit H. We go left, we visit G. G has no left child or right child, we go back up to H. And then we go to the right and we visit I, we are done with I. We're done with H. We're done with F and we're done with E. And we got the result that we were looking for. Finally we do the postorder traversal for the same example. Postorder traversal requires us to visit each of the sub trees and then to visit the root. So we start with E, but we defer E and go left. We defer B and go left. We defer A, A has no children so we visit A. We're done with B, but we can't visit the root until we've taken care of the right child. So we go right, we defer D. We go left, we defer C. C has no left child, C has no right child, we visit C, back to D. D has no right child so finally we visit D. We're all done with D so we go back to B and because we have visited both the left and right children we now visit B. We go back up to E and that takes care of left child, but now we need to do the right child before we take care of the root. So we go to the right, we differ F, F has no left child.  We defer F until we take care of the right. So we go to H, we defer H. We go left, we defer G. G has no left child, no right child so we visit G. Back up to H, we defer H. We go down to the right, we defer I. I has no left child, no right child so we visit I. Having finished with both of the children of H we visit H. We're done with that, we go back up to F and we visit F. And finally we go back up to E and we visit E. Again, hopefully we got the desired result. The code that you might write for doing the traversal could look like this. Now I've called it traverse which is rather generic, instead of like inorder traversal, or preorder traversal. Because the code is so generic I could almost write the same thing for any of the three versions. The only real difference is I take the middle line, which is where I'd want it for inorder traversal and I move it first if I'm doing preorder traversal and I move it last if I'm doing a postorder traversal. And of course I would update the line visit to reflect whatever operations I'm actually doing for my particular application.  In addition to moving a rounded tree recursively like we've just done with inorder, preorder and postorder traversal, we also want to take a look at interative methods to move around a tree. There are three reasonable choices. We can simulate recursion. Improve the results and gradually turn it into an interative piece of code. If you're interested in that you should look at the first section of the chapter on trees.  The author goes through the exercise of taking the inorder traversal method and converting it to iteration and gradually improving it until he has a completely interative piece of code. You do that by simulating recursion through the use of stacks and gradually making improvements. Keeping in mind that every time you make a change to the code there's a possibility of introducing errors. See it is a process that requires you to be very careful. Look at that both as an illustration of an interative version of the traversal, but also look at it as an example of that conversion process. We are going to look at the use of parent pointers in more detail along with the use of threaded trees.  Parent pointers work for any traversal order. Actually they allow you to move about the tree at will so you're not really locked in to any particular order. If I had a simple tree that looked like this and I wanted to build it with parent pointers, then all I really need to do-- go ahead and stick in all my actual pointers for this tree. All I really need to do is add an additional pointer field to each node that refers back to the parent. A is a node it will have a parent field and since it's the root of the tree that parent will be null. It should be very easy to modify our node declaration in our method set left and set right to accommodate and maintain this extra field.  It basically turns our multi-linked list into a doubly multi-linked list. Because it goes in both directions.  Keep in mind you do need to allocate and maintain this extra space, but it has a lot of advantages in terms of flexibility. So if you want to go down to the right child, go back up to the parent, go down to the left child, go up two nodes to the grandparent, go down to the left child again, go up to the parent, go to the right child. It's a very flexible way to move through the tree. It can be just the ticket if that's what your application needs.  All right we're going to spend a little bit more time talking about threaded trees. In particular we are going to talk about right-in-threaded trees. They only work for inorder traversal. They take advantage of space that's already being allocated, but not especially being used. And they attempt to eliminate some of the redundancy associated with typical recursive traversals.  If you look at the node that we've been using for our trees, you'll see that it requires the addition of an field which is a Boolean.  And so what we'll see on our nodes is here's a node containing the value H with two child pointers, so the Boolean is set to false, meaning no thread. Here the Boolean is set to true, meaning that this node is acting as a thread instead of a right child pointer. So the Boolean field is true if the right child pointer is a thread. And if it actually points to the child node then it is false.  Now off to the side a minute. Let's take a quick look at what happens when we do traversal of a tree. So here we are at some generic node. It has an edge coming in from its parent. It has edges going out to the two children it probably has.  So as I'm doing my inorder traversal of this tree I come down to the node, I go to the left tree, left subtree. I come back to the node. I go to the right subtree, I come back to the node and I return to the parent. On one of those three visits I'll actually do something. But I pass through three times so there's a certain amount of redundancy and there might be some benefit to getting rid of a little bit of that redundancy.  The other thing that we want to make note of is that on every leaf there is a right child pointer that's null. So it's space that's been allocated and it isn't being used for anything.  So we can take advantage of that for our threads.  Each thread is going to point to the inorder successor. The inorder successor of three is four.  The inorder successor of four is actually the other four, but we don't need a thread for that because we can get to it through our child pointers. The inorder successor of the second four is five. We can get to five's inorder successor through child pointers. The Inorder successor of the second five is the seven. The inorder successor of the seven is the first nine. We can get to the inorder successor of the first nine which is the second nine, by the child pointers. The Inorder successor of the second nine is the root 14. We can get to 15 and 14 through child pointers. The inorder successor of the second 14 is 15. For 15 we can get to it's inorder successor 16 through child pointers. We can get to the inorder successor of 16 through a child pointer. We can get-- we put in the thread to go for the inorder successor of 17 which is 18. From 18 we can go to 20 with a child pointer. And we do call the link coming out of 20 a thread because it's telling us there are no more inorder successors. Every single place that there is a null right child in this tree we replace it with a thread. So we are not left with any null right child pointers.  Now yes, on node 20 we clearly have a null right pointer, but is not a child pointer it is a thread.  It would be really messy if we had to actually go through a tree that had already been created and insert those threads. That could be computationally quite time consuming. Instead what we are going to do is incorporate them into the tree as we go along. So here we are building the tree that we've built before. We insert 14 as the root and 15 as the right child.  And the null pointer coming out of 15 is set as a null thread because there is no Inorder successor to 15.  When we add the four we need to set the inorder successor to four, but that's easy because the inorder successor will always be the parent if you're talking about a left child.  So we're repeating our tree and now we're going to add a new node, nine as the right child of four. Well if we are doing an inorder traversal of this tree the inorder traversal of the original tree looks like four, 14 and 15. The inorder traversal of this modified tree is four, nine, 14, 15. The new node if you will, has been shoved in here between the four and the 14. Which means that nine has the same inorder successor that four did in the tree on the left. So we can copy the inorder successor information from four into nine and we're good to go.  One more example. We will go ahead and insert a seven and like the four it's inorder successor is its parents and there we are. So it's easy to thread the tree if we do it as we go along. And then we can propagate the threads.  In order to make this work you do require making modifications to our methods set left, set right and make tree.  In the node declaration we need to start out with the data field, the left and the right child references. And we need to add the Boolean field for our thread. So I'm highlighting the changes to our previous instances of code by using a different color.  Here is our make tree declaration. Everything is the same. We allocate some space. We stuff in the data value. We default the left and right children to being null and we are going to default the thread field to be false.  That's not cast in stone. You can make reasonable arguments for defaulting it to be true. I would suggest you do it on a case by case basis. And just think about which version makes the most sense to be the default. OK here we're going to do the set left. So we start out like we did before, we're going to call make tree. It's going to return a temporary pointer which we assign as the left child of the particular parent. The inorder successor of that child will be the parent itself. So setting thread is very easy to do. And we just have to reset the thread field in the new node to make sure that it reflects the fact that the right child pointer is a thread and not a child. And as we did before, we copy the parent information from the private variable here and we checked to make sure that left-- the parent did not have a left child. So that part's all the same and the changes are again in white.  Now take a look at set right. That's a little bit different. We will go ahead and do our call to make tree. Notice we're going to change the order here a little bit. So we're doing a set right.  Now this is for a node that doesn't already have an existing right child, it may have a left child.  But here's the new node. We've stuffed our data value in. And before we attach it as the right child we want to copy that thread information. If we went ahead and did that first we'd over write the thread information and lose it. So we're going to copy that thread information, make sure we set the thread field to be true. Then we can go ahead and attach it which kind of gets rid of that. And we need to make sure that we set the thread field in the parent to be false. The other thing that's a little bit different here is that we need to change our test where we look and determine if there's an existing right child. There's always going to be, or almost always a node being pointed to with that right pointer. The question's going to be, is it a child? Or is that a thread we're following? The only way you can really tell is to look at the Boolean thread field. So the error check has been modified to look at the Boolean thread field to determine if there's a right child or not.  We looked at right-in-threaded trees and that's all I'm going to hold you responsible for. Trees can also be right-threaded for preorder and postorder traversal. So instead of pointing to the inorder successor, you point to the preorder successor, or the postorder successor respectively. It is possible to have a left-threaded tree where you do the same thing with the left threads that we did with the right threads. Left threads points predecessors. Inorder predecessors, preorder predecessors and postorder predecessors, depending on which type of threading you're doing. It's also possible to have both sets of threads, those are referred to as fully-threaded trees and there's a nice discussion in Horowitz and Sahni if you're interested, that's the data structures book, if you're interested in additional information on threaded trees.  We are now going to look at an example called binary op-trees. This example is an application of binary trees, but it also ties in very nicely to some material that we looked at earlier in this semester in the module on stacks.  In that module we looked at infix, postfix and prefix expressions. So I have two different expressions, they evaluate to 23 and 35. And in order to evaluate the second one corectly I use parentheses to force a different order of evaluation. I have given the postfix and prefix equivalences of those two statements and you can see that they are physically different, reflecting their different nature.  What I'm going to do is to turn the left expression into a tree. Simple operands become the leaves and the operators become the internal nodes of the tree, with the least precedence operator becoming the root of the tree. As a practical consideration you might find it simpler to build the trees by listing all of the leaves on the same level. Then you can apply the operators to them.  However, you need to do that. And when you're finished and you see how the tree actually looks you can fix it-- you can fix it and make it pretty.  We're also going to create a tree for the expression on the right. And the difference is in the two expressions are clearly reflected in the different physical structures of the tree. They have different least precedence operators and they therefore have different roots.  This kind of a tree is an example of a heterogeneous tree. The type of the leaves is different than the type of the internal vertices, or the non-leaves.  Once I have my trees, I can traverse them in preorder to get the prefix expression in order to get the infix expression and postorder to get the postfix expression. So you may want to flip back to the previous slide a minute and verify that those traversals do give you those orders.  We're going to do another example.  First of all, notice that the least precedence operator here is the dollar sign. You may recall the dollar sign is used to represent exponentiation. And we have use parentheses to force an order of evaluation that makes the dollar sign the least precedence, even though it's normally the highest precedence operator. Use this example as an opportunity to practice doing inorder, preorder and postorder traversals. Stop the video for a moment and see if you can correctly write those down.  OK here you go.  Those are your three traversal orders.  All right now, the general tree that we built earlier we put smaller items to the left and larger items to the right and this built a general, somewhat stringy tree that had no particular characteristics. Recall the tree that we built earlier using the process where smaller items went to the left and larger items went to the right. Now this was a very easy method for building a tree. And it tended to build a tree that had no special characteristics. It was just general. The leaves are not all at the same level. In fact the leaves are spread over a number of levels. it is not a regular tree, it is not a complete tree or a full tree. It's not anything special. It does however, have one interesting characteristic.  Stop the video for a minute and do the inorder traversal.  OK when you do the inorder traversal you should have gotten the entire sorted list in order three, four, four, five, five, seven, nine, nine, 14, 14, 15, 16, 17, 18 and 20.  This tree has what's called a search tree characteristic. The search tree characteristic is a recursive property that stems directly from the way we built the tree. Keep-- where we put everything in smaller in the left subtree and everything larger in the right subtree. This is a property that needs to be recursively true throughout the tree and any tree that has this characteristic will always yield the inorder-- will always yield the sorted ordering when you do the inorder traversal. We're going to make use of that fact, later on when we get to the module on searching and we talk about using trees for searching. We will always want them to have this [INAUDIBLE]. The method that we have used so far to build a tree, as we just pointed out, doesn't give us a tree with special attributes.  The tree that we created with this general method is not particularly efficient. It would be more efficient to build a tree that was as short and bushy as possible for the given number of nodes. So we'd like to find a way to build that type of a tree that has search tree characteristic and yet has this greater efficiency. That would be a nice goal. So the object is, to have something that looks like the example on the left rather than the example on the right. A term we might use to describe the tree on the left is balanced. And at this point that's really just an intuitive concept. In the module on searching we will come back and visit these trees again, but I want to introduce them to you at this point so that we can make a distinction between the two types of trees. Let's call the tree that we have been building so far a sort tree. This tree is going to be a search tree. Instead of starting with a random list it'll start with a sorted list. It will use the middle item as the root and each half will become the respective subtrees. The process for building this tree is very clearly recursive.  If we took a sorted list and applied our old process to it, we'd get something very distorted and biased. Basically a list like you see on the left. Instead if we apply this recursive process to an ordered file, the new process, we get this nice balanced tree like you see on the right. So 25 becomes the root. 42 through 96 becomes the right subtree. The middle item there is 70, that becomes the root. 42 and 43 become the left subtree. 80 and 96 becomes the right subtree. For 80 and 96 they are in positions one, two-- 10 and 11.  Or I'm sorry, nine and 10.  Back up, for 80 and 96 they are in positions nine and 10. 9 plus 10 is 19 divided by 2 is 9 1/2, rounding down tells us position nine is the midpoint. And that's the same for all of these little subtrees at the bottom. Which is why you see this sort of bias to the right. But the claim is, is this tree is as short and bushy as possible for the number of items. There are others that are equally short and bushy, but none that are better. So here is a little comparison between these two strategies. So that you have them clear in your mind and when you're asked for a sort tree or your asked for a simple search tree, you know which one is the one that we're interested in, in that particular situation.  With any tree we need to be able to do insertions and deletions. So we're going to insert like we did before and we looked at that when we were looking at the code examples. This is the point at which we are going to illustrate deletion from a binary tree. There are three cases for deletion. No children, one child, or two children. As with any structure, as you make changes to it because data is dynamic and we have to insert and delete items over time. This particular structure is going to deteriorate and become more generalized. When the performance becomes poor we need to expect to rebuild it.  If we take the example that we had before and we wanted to delete the 43. 43 was just a leaf. The hole that's created by its deletion is down at the bottom level, not a big deal. So if we want to get rid of a leaf we just get rid of it. Next we're going to put the 43 back where it belongs and delete the 42.  Think about what we might do with the 42.  If we delete the 42 where it belongs that creates a hole at this level of the tree. So what we're going to do is fill that hole by promoting the 43. And we delete the 43 from its original spot which we already know how to do. And the whole that is created by deleting the 42 is pushed down to the bottom level of the tree.  This example shows the deletion of the 70. We have replaced it with the 80. The hole that's created by the 80 is filled with the 96. The 96 is promoted to-- from its spot and the hole that's created by deleting the 70 is again down at the very bottom level. So this strategy, it does tend to minimize the effects of the deletion, but it can't completely eliminate them, but it does help. OK now we're going to do something a little bit more difficult. Suppose we want to delete the 25. Let's assume the 70, the 80 and the 96 are back in their original positions and we want to delete the 25. Stop the video for a moment and give some thought to what's going to take the 25s place.  OK if you're ready to continue, we've replaced 25 with the 42. Replacing it with the 15 or the 70 would be difficult because 15 and 70 each have two children. Why did we pick the 42?  We picked the 42 because it is the inorder successor of the 25. We could just as readily have chosen the 22. The 22 is the inorder predecessor of the 25. Either one would work perfectly well, but we need to have consistent and reproducible results. So our policy is going to be to use the inorder successor.  The inorder successor will only be a leaf, or have one child. The inorder successor will never have two children. If it did, it wouldn't be the inorder successor. So we replace the 25 with the 42. The 42 is replaced with the 43 just like earlier. The 43 is promoted and the hole that's created even though we're deleting the root of the tree, is pushed all the way down to the very bottom level. Thus minimizing the effects of the deletion.  We will come back and revisit binary trees in the module on searching.