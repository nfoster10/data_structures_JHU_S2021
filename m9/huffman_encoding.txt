 Welcome to the module on Huffman Encoding. Huffman encoding is a nice example of the application of binary trees. It also represents an excellent venue for coding up some of the various implementations that we've discussed throughout the course of the semester. It's a good example of data compression. And in addition to that, it's a good example of the importance of understanding the interpretation roles for a data structure. If you don't know the correct rules, contents are basically garbage. If you're like most people, as a child you may well have had a best friend with whom you exchanged secret messages. Heaven forbid, your younger brother or sister would figure out what you were up to. Or a teacher read a note that may have gotten dropped at school. And typically what you would do is what's called a substitution cipher. In a substitution cipher, A becomes B, B becomes C, and so on and so forth. If you were a little bit more sophisticated, maybe you substituted A for Z, B for Y, and so on and so forth. But the basic problem with ciphers is that they are subject to both frequency and pattern analysis. We'll seen an example on a later slide. Another thing you could have done would have been to create a code. Codes require a key. Because what codes do is they take an expression and assign an arbitrary meaning to it that's completely different from its regular meaning. So in order to decode them, you have to have the key so you can look things up. The key then becomes something else that must be safe guarded and shared. There may be multiple copies floating around so that different people can use the code. A phrase like, meet me at Uncle Fred's house on Saturday at 3, might actually mean something like, meet me down at the dock on Sunday at 1. These types of messages are very difficult to decode because they're not subject to frequency analysis or pattern analysis like ciphers can be. Here's a link if you're interested in reading a little more about ciphers and codes. As I mentioned, Huffman encoding turns out in practice to actually be useful for data compression. Although it did start out as an encoding technique, it has been long surpassed by more modern and sophisticated mathematical methods. What happens with Huffman encoding is that the most frequently used letters will get the shortest codes and in contrast, the least frequently used letters will get the longest codes. The messages that you get with Huffman encoding are not going to be subject to frequency analysis. Anyone who's a fan of the Wheel of Fortune knows that E is the most commonly used letter of the alphabet. So if you simply count up all of the symbols in a coded message, there's a high probability that the most frequently occurring symbol is an E. And if it's not an E, it's going to be one of the ones in this next group here. The second, third, fourth, fifth most commonly occurring symbols are also going to be in this group that I've underlined. So what frequency analysis does for you is it drastically reduces the size of the set that you have to try out in order to decode your message. You don't have to try out all 26 letters. Word patterns are another way that messages can easily be decoded. In this message that we have here, Q is occurring by itself as a word. That means it's probably an A or an I, as those are the only two letters that occur by themselves as a single letter word. XBC is a three letter word and there's a very good chance that the C is actually an E. Because there are limited number of three letter words that we typically use and they tend to be words like, the, she, etc. So that gives you good cause to guess, That C is really E. So good examples of what we mean by pattern and frequency analysis. As it turns out, no matter how many symbols you have in your alphabet, Huffman encoding only uses two symbols, 0 and 1, regardless, Of the size of your alphabet. It enables you to avoid pattern analysis and word pattern analysis and frequency pattern analysis. So let's look at an example. We're going to keep it simple and only use a four letter alphabet, A, B, C, D. And let's suppose that we know the frequencies associated with these letters, and we interpret our frequency table as follows. B and D are the least frequently used letters so they have a frequency of 1. And A is the most frequently used letter, and it has frequency of 3. It is used three times as often as either B or D. C appears to be used twice as often as B or D. So the first thing you'll notice are the codes without worrying about how we've gotten them yet. The code for A is a single bit. It's the shortest, which corresponds to the fact that it's the most frequently used. B and D are the longest, and they are the least frequently used, so that corresponds to what we've been told to expect. Suppose we have a simple message that we want to encode, ABABBA. Looking at our list, we encode it as A is 0, B is 110, A is 0, B is 110, B is 110. And our final A is 0. If we count that up, we can see there are exactly 12 bits used in the encoding of this message. If we use normal ASCII or [INAUDIBLE] coding, where each letter has an 8 bit symbol or a full byte, then our six letter message would take 48 bits. So you can see even in this simple example, which doesn't use all the letters, we have achieved a significant amount of compression. Which is why Huffman encoding is considered a compression technique. All right, now we're going to take our simple alphabet and show how we turn that into a code using a binary tree. This is an example of what we call a greedy strategy. Now that term's not important for the purposes of this course but it's a term that you're likely to run into at a later point. And a greedy strategy is one in which we only make locally optimal choices. We don't worry about whether they're globally optimal, just whether they're locally optimal. In this case, that means starting with the two least frequently available letters of the alphabet. So that's B and D. And we turn those into nodes that become leaves on our binary tree. The nodes are labeled with the letter of the alphabet and the corresponding frequency. In addition to that, we create a parent above those nodes. And the label of the parent is the combined alphabetic label as well as the combined frequency label. So those two items get crossed off our list. And the combined results is entered back into the list. We take the two least frequently used remaining items. That would be C and DB. And we create more nodes and add them to our tree and enter BCD4 back into our list. We take the two least frequently remaining items and we combine them to create a new and final node ABCD7. That's our binary tree. At this point, you might like to stop for a minute and give a little bit of thought to how you figure out, Why B went to the left and D went to the right, and why we didn't do it the other way around. Why did we put C to the left instead of BD? Why did we put A to the left instead of BCD? Okay, now, we're back, When you build a Huffman encoding tree you have to have a protocol for resolving ties and for deciding which nodes you put to the left and which nodes you put to the right. Our protocol is going to be that smaller values always go to the left, like they do here. If we have a tie, then we have to have a tiebreaker. So for C and BD, those are tied, so we break the tie by putting the simpler one to the left. With B and D, they're both simple. But if they're both simple or they're both complex, then we use alphabetic ordering to resolve our tie. So it doesn't really matter what your protocol is. It matters that you have one and that everybody has agreed on it. Okay, now that we have built our tree, what do we do with it? We assign 0s and 1s to the branches. 0s to the left branches, 1s to the right branches. That allows us to follow paths from the root of the tree to the leaves. And every time we hit a leaf, we have the code for that leaf. So if we start at the root and we go right, right, left. Then that gives us 110 is the code for B. Because of the way we chose our items to be on the tree. We chose the least frequently used items first, so they're down at the bottom of the tree and have the longest code. The most frequently used items were chosen last and they're up at the top of the tree and have the shortest codes. You should easily be able to see that if we'd use different protocols for resolving the ties, we would have ended up with different codes. Suppose we had a message? That's the encoded version of that particular message. Suppose we had a message and we wanted to decode it. Well we simply use that as path information. So we start at the root of the tree. 0 tells us to go left. We hit a leaf, this must correspond to an A. We start over at the root, 1 we go right, 1 we go right. 0 we go left, 110 that puts us on B, and so on and so forth. So since we get coded strings that are not broken up to correspond to symbols, we don't get any patterns that we can easily decipher. That's the basic idea. There are some additional considerations. First of all, how do you get that pesky frequency table? Well, you simply take an agreed upon common resource. This could be the collected works of William Shakespeare. It could be the King James version of the Bible. It could be a popular novel. Doesn't matter what it is as long as you agree on it. Then you simply count the occurrence of each letter in that reference work and you create your frequency table. If you are going to have messages that have a very specific vocabulary, then it might be a good idea to use a text source that reflects that vocabulary. So for instance, if you were going to send a lot of messages on engineering topics, then some sort of standard engineering reference might be a good idea. Because it would make the most frequently occurring letters correspond to the most frequently occurring letters in your actual messages and you'd get the greatest benefit of compression. You always start by choosing the two least frequently available items remaining. It's important to agree on a protocol for determining which goes left and which goes right, putting the smaller one to the left is very typical. There is more variation on how you handle ties. We were putting the smaller nodes to the left and in this context I don't mean frequency smaller, I mean simpler. Alphabetic ordering is another way to resolve the ties. And in our example we used both, but you don't necessarily have to. You could just always use alphabetic ordering. And of course, you build the tree from the bottom up.